{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 3.ProjectedGradientDescent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsvgCPv17t3u",
        "colab_type": "code",
        "outputId": "ec0c2cf2-80fd-471f-a07c-c168626e8345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install adversarial-robustness-toolbox\n",
        "!git clone https://github.com/tensorflow/cleverhans.git\n",
        "!pip install cleverhans/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: adversarial-robustness-toolbox in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: scikit-learn==0.22.1 in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox) (0.22.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox) (1.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox) (3.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox) (46.1.3)\n",
            "Requirement already satisfied: Pillow==7.0.0 in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox) (7.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.1->adversarial-robustness-toolbox) (0.14.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox) (2.8.1)\n",
            "fatal: destination path 'cleverhans' already exists and is not an empty directory.\n",
            "Processing ./cleverhans\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (1.3.7)\n",
            "Requirement already satisfied: pycodestyle in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (2.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (3.2.1)\n",
            "Requirement already satisfied: mnist~=0.2 in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (1.18.3)\n",
            "Requirement already satisfied: tensorflow-probability in /tensorflow-1.15.2/python3.6 (from cleverhans==3.0.1) (0.7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (0.14.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans==3.0.1) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans==3.0.1) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans==3.0.1) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans==3.0.1) (1.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans==3.0.1) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans==3.0.1) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans==3.0.1) (4.4.2)\n",
            "Building wheels for collected packages: cleverhans\n",
            "  Building wheel for cleverhans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cleverhans: filename=cleverhans-3.0.1-cp36-none-any.whl size=253453 sha256=7632c3aaff7cac6fe8ffc24c21771e3004467aa80df8605ebe6deb841e8e521c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zd0or1nv/wheels/d1/6b/1d/5cf7b3ca4c0cfc7f845628b8ed46366ab5f4f56b5483e9db7f\n",
            "Successfully built cleverhans\n",
            "Installing collected packages: cleverhans\n",
            "  Found existing installation: cleverhans 3.0.1\n",
            "    Uninstalling cleverhans-3.0.1:\n",
            "      Successfully uninstalled cleverhans-3.0.1\n",
            "Successfully installed cleverhans-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT3K5k-fIIZ0",
        "colab_type": "code",
        "outputId": "18656f6a-6ea9-4006-d926-8da32350f56d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from art.attacks import ProjectedGradientDescent\n",
        "from art.classifiers import KerasClassifier\n",
        "from art.utils import load_dataset\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Load the CIFAR-10 Dataset\n",
        "(x_train, y_train), (x_test, y_test), min_, max_ = load_dataset(str(\"cifar10\")) # Original Dataset\n",
        "print(\"x_train shape: \" + str(x_train.shape) + \"\\n\" + \"x_train size: \" + str(x_train.size) + \"\\n\" + # this print statement is used for understanding what the CIFAR-10 dataset is\n",
        "      \"y_train shape: \" + str(y_train.shape) + \"\\n\" + \"y_train size: \" + str(y_train.size) + \"\\n\" +\n",
        "      \"x_test shape: \" + str(x_test.shape) + \"\\n\" + \"x_test size: \" + str(x_test.size) + \"\\n\" +\n",
        "      \"y_test shape: \" + str(y_test.shape) + \"\\n\" + \"y_test size: \" + str(y_test.size) + \"\\n\")\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Load the victim model\n",
        "classifier_url =\"https://tfhub.dev/deepmind/ganeval-cifar10-convnet/1\" #@param {type:\"string\"} # model is downloaded from this site\n",
        "IMAGE_SHAPE = (32, 32) # the image shape is needed so that the model knows the input-shape and since we are working with the CIFAR-10 all the images are 32 x 32 color images\n",
        "classifier = KerasClassifier(model=tf.keras.Sequential([hub.KerasLayer(classifier_url, input_shape=IMAGE_SHAPE+(3,))]), clip_values=(min_, max_)) # this bascially creates a keras wrapper around the downloaded model so that we can use it with keras functions.\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Evaluate the victim model on the benign dataset\n",
        "predictions = classifier.predict(x_test) # giving the classifier the x_test of the CIFAR-10 dataset.\n",
        "accuracy_benign = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test) # calculates the accuracy of the predictions\n",
        "print(\"Accuracy on benign test examples: {}%\\n\".format(accuracy_benign * 100))\n",
        "\n",
        "\n",
        "\n",
        "# Step 4: Collect 10 instances of each class from test set\n",
        "def exract_ten_classes(data, labels, classes=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), no_instance=10):\n",
        "    x_pre = []  # list to collect the x_test set\n",
        "    y_pre = []  # list to collect the y_test set\n",
        "    for class_label in range(0, 10):  # loop through each of the classes\n",
        "        index = random.randint(0, 5000)  # choose an index from the x_test\n",
        "        iteration = no_instance  # number of instance of each class to collect\n",
        "        while (iteration != 0):\n",
        "            if np.argmax(labels[index]) == classes[class_label]:  # check if the current index label matches the specified class label we are looking for\n",
        "                x_pre.append(data[index])  # add the image to the x_test set\n",
        "                y_pre.append(int(class_label))  # add the image label to the y_test set\n",
        "                iteration = iteration - 1  # reduce # of instances by 1\n",
        "            index = index + 1  # go to next index till next label is of the current class\n",
        "    x = np.asarray(x_pre)  # append all 100, 10 of each class, images together\n",
        "    y = keras.utils.to_categorical(np.asarray(y_pre), 10)  # append all 100, 10 of each class, labels together and do one hot encoding\n",
        "    return x, y\n",
        "\n",
        "x_test_adv_pre, y_test_adv = exract_ten_classes( x_test, y_test )\n",
        "print(\"x_test_adv_pre shape: \" + str(x_test_adv_pre.shape) + \"\\n\" + \"x_test_adv_pre size: \" + str(x_test_adv_pre.size) + \"\\n\" +\n",
        "      \"y_test_adv_pre shape: \" + str(y_test_adv.shape) + \"\\n\" + \"y_test_adv_pre size: \" + str(y_test_adv.size) + \"\\n\")\n",
        "\n",
        "\n",
        "# Step 6: Generate adversarial test examples and Evaluate the ART classifier on adversarial test examples\n",
        "attack_eps_5 = ProjectedGradientDescent(classifier=classifier, eps=0.1, norm=2, max_iter=90)\n",
        "x_test_adv_eps_5 = attack_eps_5.generate(x=x_test_adv_pre)\n",
        "predictions_eps_5 = classifier.predict(x_test_adv_eps_5)\n",
        "accuracy_adv_eps_5 = np.sum(np.argmax(predictions_eps_5, axis=1) == np.argmax(y_test_adv, axis=1)) / len(y_test_adv)\n",
        "#print(\"Accuracy on adversarial test examples with eps = 0.05: {}%\".format(accuracy_adv_eps_5 * 100))\n",
        "\n",
        "attack_eps_10 = ProjectedGradientDescent(classifier=classifier, eps=0.25,  max_iter=100)\n",
        "x_test_adv_eps_10 = attack_eps_10.generate(x=x_test_adv_pre)\n",
        "predictions_eps_10 = classifier.predict(x_test_adv_eps_10)\n",
        "accuracy_adv_eps_10 = np.sum(np.argmax(predictions_eps_10, axis=1) == np.argmax(y_test_adv, axis=1)) / len(y_test_adv)\n",
        "#print(\"Accuracy on adversarial test examples with eps = 0.1: {}%\".format(accuracy_adv_eps_10 * 100))\n",
        "\n",
        "attack_eps_50 = ProjectedGradientDescent(classifier=classifier, eps=0.5,  max_iter=110)\n",
        "x_test_adv_eps_50 = attack_eps_50.generate(x=x_test_adv_pre)\n",
        "predictions_eps_50 = classifier.predict(x_test_adv_eps_50)\n",
        "accuracy_adv_eps_50 = np.sum(np.argmax(predictions_eps_50, axis=1) == np.argmax(y_test_adv, axis=1)) / len(y_test_adv)\n",
        "#print(\"Accuracy on adversarial test examples with eps = 0.5: {}%\".format(accuracy_adv_eps_50 * 100))\n",
        "\n",
        "attack_eps_95 = ProjectedGradientDescent(classifier=classifier, eps=0.99, max_iter=120)\n",
        "x_test_adv_eps_95 = attack_eps_95.generate(x=x_test_adv_pre)\n",
        "predictions_eps_95 = classifier.predict(x_test_adv_eps_95)\n",
        "accuracy_adv_eps_95 = np.sum(np.argmax(predictions_eps_95, axis=1) == np.argmax(y_test_adv, axis=1)) / len(y_test_adv)\n",
        "#print(\"Accuracy on adversarial test examples with eps = 0.95: {}%\".format(accuracy_adv_eps_95 * 100))\n",
        "\n",
        "accuracies = [accuracy_adv_eps_5 * 100, accuracy_adv_eps_10 * 100, accuracy_adv_eps_50 * 100, accuracy_adv_eps_95 * 100]\n",
        "\n",
        "\n",
        "# Step 7: Plot Results\n",
        "for ind in range(0, 100, 5):\n",
        "    fig = plt.figure(figsize=(16, 16))\n",
        "    fig.suptitle('Adversarial Attack On Victim Model', fontsize=24, fontweight='bold')\n",
        "    columns = 5\n",
        "    rows = 7\n",
        "    ax = []\n",
        "\n",
        "    ax.append(fig.add_subplot(rows, columns, 1))\n",
        "    plt.text(0.38, 0.1, 'Original Image', fontsize=10, fontweight='bold')\n",
        "    plt.axis('off')\n",
        "\n",
        "    eps = [0.05, 0.1, 0.5, 0.95]\n",
        "    for i in range(2, 6):\n",
        "        ax.append(fig.add_subplot(rows, columns, i))\n",
        "        plt.text(0.0, 0.1, 'Adversarial Image EPS = ' + str(eps[i - 2]), fontsize=10, fontweight='bold')\n",
        "        plt.axis('off')\n",
        "\n",
        "    imageindex = ind\n",
        "    for i in range(5, columns*rows - 6, 5):\n",
        "        sample_pre = x_test_adv_pre[ imageindex, :]\n",
        "        ax.append( fig.add_subplot(rows, columns, i + 1) )\n",
        "        label_pre = np.argmax(classifier.predict(sample_pre.reshape((1, sample_pre.shape[0], sample_pre.shape[1], sample_pre.shape[2]))))\n",
        "        plt.text(33, 18, 'Data:\\nTrue Label = %d\\nPredicted Label = %d' % (np.argmax(y_test_adv[imageindex]), label_pre))\n",
        "        plt.imshow(sample_pre)\n",
        "\n",
        "        sample_post_eps_5 = x_test_adv_eps_5[ imageindex, :]\n",
        "        ax.append( fig.add_subplot(rows, columns, i + 2) )\n",
        "        label_post_eps_5 = np.argmax(classifier.predict(sample_post_eps_5.reshape((1, sample_post_eps_5.shape[0], sample_post_eps_5.shape[1], sample_post_eps_5.shape[2]))))\n",
        "        plt.text(33, 18, 'Data:\\nTrue Label = %d\\nPredicted Label = %d' % (np.argmax(y_test_adv[imageindex]), label_post_eps_5))\n",
        "        plt.imshow(sample_post_eps_5)\n",
        "\n",
        "        sample_post_eps_10 = x_test_adv_eps_10[imageindex, :]\n",
        "        ax.append(fig.add_subplot(rows, columns, i + 3))\n",
        "        label_post_eps_10 = np.argmax(classifier.predict(sample_post_eps_10.reshape((1, sample_post_eps_10.shape[0], sample_post_eps_10.shape[1], sample_post_eps_10.shape[2]))))\n",
        "        plt.text(33, 18, 'Data:\\nTrue Label = %d\\nPredicted Label = %d' % (np.argmax(y_test_adv[imageindex]), label_post_eps_10))\n",
        "        plt.imshow(sample_post_eps_10)\n",
        "\n",
        "        sample_post_eps_50 = x_test_adv_eps_50[imageindex, :]\n",
        "        ax.append(fig.add_subplot(rows, columns, i + 4))\n",
        "        label_post_eps_50 = np.argmax(classifier.predict(sample_post_eps_50.reshape((1, sample_post_eps_50.shape[0], sample_post_eps_50.shape[1], sample_post_eps_50.shape[2]))))\n",
        "        plt.text(33, 18, 'Data:\\nTrue Label = %d\\nPredicted Label = %d' % (np.argmax(y_test_adv[imageindex]), label_post_eps_50))\n",
        "        plt.imshow(sample_post_eps_50)\n",
        "\n",
        "        sample_post_eps_95 = x_test_adv_eps_95[imageindex, :]\n",
        "        ax.append(fig.add_subplot(rows, columns, i + 5))\n",
        "        label_post_eps_95 = np.argmax(classifier.predict(sample_post_eps_95.reshape((1, sample_post_eps_95.shape[0], sample_post_eps_95.shape[1], sample_post_eps_95.shape[2]))))\n",
        "        plt.text(33, 18, 'Data:\\nTrue Label = %d\\nPredicted Label = %d' % (np.argmax(y_test_adv[imageindex]), label_post_eps_95))\n",
        "        plt.imshow(sample_post_eps_95)\n",
        "\n",
        "        imageindex = imageindex + 1\n",
        "\n",
        "\n",
        "    ax.append(fig.add_subplot(rows, columns, 31))\n",
        "    plt.text(0.0, 0.5, \"Accuracy on benign test examples: {}%\".format(round(accuracy_benign * 100),2), fontsize=8, fontweight='bold')\n",
        "    plt.axis('off')\n",
        "\n",
        "    for i in range(32, 36):\n",
        "        ax.append(fig.add_subplot(rows, columns, i))\n",
        "        plt.text(0.0, 0.5, \"Accuracy on test examples eps = \" + str(eps[i-32]) + \": {}%\".format(round(accuracies[i-32]), 2), fontsize=8, fontweight='bold')\n",
        "        plt.axis('off')\n",
        "\n",
        "    fig.tight_layout(h_pad=4.0, w_pad=4.0)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Step 9: Data from Results\n",
        "print()\n",
        "print(\"Accuracy on benign test examples: {}%\".format(accuracy_benign * 100))\n",
        "print(\"Accuracy on adversarial test examples with eps = 0.05: {}%\".format(accuracies[0]))\n",
        "print(\"Accuracy on adversarial test examples with eps = 0.1: {}%\".format(accuracies[1]))\n",
        "print(\"Accuracy on adversarial test examples with eps = 0.5: {}%\".format(accuracies[2]))\n",
        "print(\"Accuracy on adversarial test examples with eps = 0.95: {}%\".format(accuracies[3]))\n",
        "print()\n",
        "\n",
        "all_count = []\n",
        "for j in range(0, 100, 10):\n",
        "    count = [0, 0, 0, 0, 0]\n",
        "    for i in range(j, j + 10):\n",
        "        sample_pre = x_test_adv_pre[ i, : ]\n",
        "        label_pre = np.argmax(classifier.predict(sample_pre.reshape((1, sample_pre.shape[0], sample_pre.shape[1], sample_pre.shape[2]))))\n",
        "        sample_post_eps_5 = x_test_adv_eps_5[ i, : ]\n",
        "        label_post_eps_5 = np.argmax(classifier.predict(sample_post_eps_5.reshape((1, sample_post_eps_5.shape[0], sample_post_eps_5.shape[1], sample_post_eps_5.shape[2]))))\n",
        "        sample_post_eps_10 = x_test_adv_eps_10[ i, : ]\n",
        "        label_post_eps_10 = np.argmax(classifier.predict(sample_post_eps_10.reshape((1, sample_post_eps_10.shape[0], sample_post_eps_10.shape[1], sample_post_eps_10.shape[2]))))\n",
        "        sample_post_eps_50 = x_test_adv_eps_50[ i, : ]\n",
        "        label_post_eps_50 = np.argmax(classifier.predict(sample_post_eps_50.reshape((1, sample_post_eps_50.shape[0], sample_post_eps_50.shape[1], sample_post_eps_50.shape[2]))))\n",
        "        sample_post_eps_95 = x_test_adv_eps_95[ i, : ]\n",
        "        label_post_eps_95 = np.argmax(classifier.predict(sample_post_eps_95.reshape((1, sample_post_eps_95.shape[0], sample_post_eps_95.shape[1], sample_post_eps_95.shape[2]))))\n",
        "        if (label_pre == np.argmax(y_test_adv[i])):\n",
        "            count[0] = count[0] + 1\n",
        "        if (label_post_eps_5 == np.argmax(y_test_adv[i])):\n",
        "            count[1] = count[1] + 1\n",
        "        if (label_post_eps_10 == np.argmax(y_test_adv[i])):\n",
        "            count[2] = count[2] + 1\n",
        "        if (label_post_eps_50 == np.argmax(y_test_adv[i])):\n",
        "            count[3] = count[3] + 1\n",
        "        if (label_post_eps_95 == np.argmax(y_test_adv[i])):\n",
        "            count[4] = count[4] + 1\n",
        "    all_count.append(count)\n",
        "print()\n",
        "\n",
        "Labels = [\"Airplane\", \"Automobile\", \"Bird\", \"Cat\", \"Deer\", \"Dog\", \"Frog\", \"Horse\", \"Ship\", \"Truck\"]\n",
        "for i in range(0, 10):\n",
        "    print(\"Classifier with benign example has \" + str(Labels[i]) + \" recognition accuracy of = \" + str((all_count[i][0] / 10) * 100) + \"%\")\n",
        "    print(\"Fast Gradient Method with eps = 0.05 has \" + str(Labels[i]) + \" recognition accuracy of = \" + str((all_count[i][1] / 10) * 100) + \"%\")\n",
        "    print(\"Fast Gradient Method with eps = 0.10 has \" + str(Labels[i]) + \" recognition accuracy of = \" + str((all_count[i][2] / 10) * 100) + \"%\")\n",
        "    print(\"Fast Gradient Method with eps = 0.50 has \" + str(Labels[i]) + \" recognition accuracy of = \" + str((all_count[i][3] / 10) * 100) + \"%\")\n",
        "    print(\"Fast Gradient Method with eps = 0.95 has \" + str(Labels[i]) + \" recognition accuracy of = \" + str((all_count[i][4] / 10) * 100) + \"%\")\n",
        "    print()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzgUz6pONdb0",
        "colab_type": "text"
      },
      "source": [
        "Defense\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfFbPwbsNf_6",
        "colab_type": "code",
        "outputId": "2e24891c-45d2-45a6-f1f6-ce645084a8f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "!pip install git+https://github.com/nottombrown/imagenet_stubs\n",
        "\n",
        "import imagenet_stubs\n",
        "from imagenet_stubs.imagenet_2012_labels import name_to_label, label_to_name\n",
        "from art.defences import JpegCompression\n",
        "from art.defences import TotalVarMin\n",
        "from art.defences import GaussianNoise\n",
        "from art.defences import PixelDefend\n",
        "from art.defences import ClassLabels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/nottombrown/imagenet_stubs\n",
            "  Cloning https://github.com/nottombrown/imagenet_stubs to /tmp/pip-req-build-su0mdy8i\n",
            "  Running command git clone -q https://github.com/nottombrown/imagenet_stubs /tmp/pip-req-build-su0mdy8i\n",
            "Requirement already satisfied (use --upgrade to upgrade): imagenet-stubs==0.0.7 from git+https://github.com/nottombrown/imagenet_stubs in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: imagenet-stubs\n",
            "  Building wheel for imagenet-stubs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imagenet-stubs: filename=imagenet_stubs-0.0.7-cp36-none-any.whl size=794841 sha256=58c11b042d37844526dcc0a8aa3f1abe1323c2d0c2b4effd974af9b51ac40fef\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vxihckm7/wheels/ae/51/e7/c1cfa8692ea864410aa778b169fc7766af073ac52a1bc23301\n",
            "Successfully built imagenet-stubs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU3LLP4mN1nf",
        "colab_type": "code",
        "outputId": "a3860bdf-225a-4876-8a8b-4f6445c2941a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        }
      },
      "source": [
        "ss = JpegCompression(clip_values=(min_, max_))\n",
        "\n",
        "x_art_def, _ = ss(x_test)\n",
        "x_art_adv_def, _ = ss(x_test_adv_eps_5)\n",
        "\n",
        "# Compute the classifier predictions on the preprocessed inputs:\n",
        "pred_def = classifier.predict(x_art_def)\n",
        "label_def = np.argmax(pred_def, axis=1)[0]\n",
        "confidence_def = pred_def[:, label_def][0]\n",
        "\n",
        "pred_adv_def = classifier.predict(x_art_adv_def)\n",
        "label_adv_def = np.argmax(pred_adv_def, axis=1)[0]\n",
        "confidence_adv_def = pred_adv_def[:, label_adv_def][0]\n",
        "\n",
        "print(\"Defense: JPEG Compression \")\n",
        "print('Prediction of original sample:', label_to_name(label_def), '- confidence {0:.2f}'.format(confidence_def))\n",
        "print('Prediction of adversarial sample:', label_to_name(label_adv_def), \n",
        "      '- confidence {0:.2f}'.format(confidence_adv_def))\n",
        "plt.figure(figsize=(8,8)); plt.imshow(x_art_adv_def[0][..., ::-1] / 255); plt.axis('off'); plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defense: JPEG Compression \n",
            "Prediction of original sample: tiger shark, Galeocerdo cuvieri - confidence 6.12\n",
            "Prediction of adversarial sample: tench, Tinca tinca - confidence 8.74\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEWvHNRnlHXV",
        "colab_type": "code",
        "outputId": "c3eebf29-d4ff-4585-fc43-3cf8c556173e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "ss = GaussianNoise()\n",
        "\n",
        "x_art_def = ss(x_test)\n",
        "x_art_adv_def = ss(x_test_adv_eps_5)\n",
        "\n",
        "# Compute the classifier predictions on the preprocessed inputs:\n",
        "pred_def = classifier.predict(x_art_def)\n",
        "label_def = np.argmax(pred_def, axis=1)[0]\n",
        "confidence_def = pred_def[:, label_def][0]\n",
        "\n",
        "pred_adv_def = classifier.predict(x_art_adv_def)\n",
        "label_adv_def = np.argmax(pred_adv_def, axis=1)[0]\n",
        "confidence_adv_def = pred_adv_def[:, label_adv_def][0]\n",
        "\n",
        "print(\"Defense: Gaussian Noise\")\n",
        "print('Prediction of original sample:', label_to_name(label_def), '- confidence {0:.2f}'.format(confidence_def))\n",
        "print('Prediction of adversarial sample:', label_to_name(label_adv_def), \n",
        "      '- confidence {0:.2f}'.format(confidence_adv_def))\n",
        "plt.figure(figsize=(8,8)); plt.imshow(x_art_adv_def[0][..., ::-1] / 255); plt.axis('off'); plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Defense: Gaussian Noise\n",
            "Prediction of original sample: stingray - confidence 10.96\n",
            "Prediction of adversarial sample: stingray - confidence 10.36\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7nW2csLlLqv",
        "colab_type": "code",
        "outputId": "30d9c318-bb55-4afa-cfcb-fe2d0e2d7800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        }
      },
      "source": [
        "ss = TotalVarMin(clip_values=(min_, max_))\n",
        "x_art_def, _ = ss(x_test)\n",
        "x_art_adv_def, _ = ss(x_test_adv_eps_5)\n",
        "\n",
        "# Compute the classifier predictions on the preprocessed inputs:\n",
        "pred_def = classifier.predict(x_art_def)\n",
        "label_def = np.argmax(pred_def, axis=1)[0]\n",
        "confidence_def = pred_def[:, label_def][0]\n",
        "\n",
        "pred_adv_def = classifier.predict(x_art_adv_def)\n",
        "label_adv_def = np.argmax(pred_adv_def, axis=1)[0]\n",
        "confidence_adv_def = pred_adv_def[:, label_adv_def][0]\n",
        "\n",
        "print(\"Defense: Total Variance Minimization\")\n",
        "print('Prediction of original sample:', label_to_name(label_def), '- confidence {0:.2f}'.format(confidence_def))\n",
        "print('Prediction of adversarial sample:', label_to_name(label_adv_def), \n",
        "      '- confidence {0:.2f}'.format(confidence_adv_def))\n",
        "plt.figure(figsize=(8,8)); plt.imshow(x_art_adv_def[0][..., ::-1] / 255); plt.axis('off'); plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defense: Total Variance Minimization\n",
            "Prediction of original sample: tiger shark, Galeocerdo cuvieri - confidence 6.84\n",
            "Prediction of adversarial sample: tench, Tinca tinca - confidence 7.38\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxaUhVkJlLu2",
        "colab_type": "code",
        "outputId": "6d5cca58-3d66-4347-a4e9-057c10b1a18c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        }
      },
      "source": [
        "ss = ClassLabels()\n",
        "\n",
        "x_art_def = ss(x_test)\n",
        "x_art_adv_def = ss(x_test_adv_eps_5)\n",
        "\n",
        "# Compute the classifier predictions on the preprocessed inputs:\n",
        "pred_def = classifier.predict(x_art_def)\n",
        "label_def = np.argmax(pred_def, axis=1)[0]\n",
        "confidence_def = pred_def[:, label_def][0]\n",
        "\n",
        "pred_adv_def = classifier.predict(x_art_adv_def)\n",
        "label_adv_def = np.argmax(pred_adv_def, axis=1)[0]\n",
        "confidence_adv_def = pred_adv_def[:, label_adv_def][0]\n",
        "\n",
        "print(\"Defense: Class Labels Compression \")\n",
        "print('Prediction of original sample:', label_to_name(label_def), '- confidence {0:.2f}'.format(confidence_def))\n",
        "print('Prediction of adversarial sample:', label_to_name(label_adv_def), \n",
        "      '- confidence {0:.2f}'.format(confidence_adv_def))\n",
        "plt.figure(figsize=(8,8)); plt.imshow(x_art_adv_def[0][..., ::-1] / 255); plt.axis('off'); plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defense: Class Labels Compression \n",
            "Prediction of original sample: tiger shark, Galeocerdo cuvieri - confidence 6.70\n",
            "Prediction of adversarial sample: tiger shark, Galeocerdo cuvieri - confidence 6.70\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYjZLbmF4pka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/nottombrown/imagenet_stubs\n",
        "\n",
        "import imagenet_stubs\n",
        "from imagenet_stubs.imagenet_2012_labels import name_to_label, label_to_name\n",
        "from art.defences import JpegCompression\n",
        "from art.defences import TotalVarMin\n",
        "from art.defences import GaussianNoise\n",
        "from art.defences import PixelDefend\n",
        "from art.defences import ClassLabels\n",
        "\n",
        "ss = JpegCompression(clip_values=(min_, max_))\n",
        "\n",
        "x_art_def, _ = ss(x_test)\n",
        "x_art_adv_def, _ = ss(x_test_adv_eps_5)\n",
        "\n",
        "# Compute the classifier predictions on the preprocessed inputs:\n",
        "pred_def = classifier.predict(x_art_def)\n",
        "label_def = np.argmax(pred_def, axis=1)[0]\n",
        "confidence_def = pred_def[:, label_def][0]\n",
        "\n",
        "pred_adv_def = classifier.predict(x_art_adv_def)\n",
        "label_adv_def = np.argmax(pred_adv_def, axis=1)[0]\n",
        "confidence_adv_def = pred_adv_def[:, label_adv_def][0]\n",
        "\n",
        "print(\"Defense: JPEG Compression \")\n",
        "print('Prediction of original sample:', label_to_name(label_def), '- confidence {0:.2f}'.format(confidence_def))\n",
        "print('Prediction of adversarial sample:', label_to_name(label_adv_def), \n",
        "      '- confidence {0:.2f}'.format(confidence_adv_def))\n",
        "plt.figure(figsize=(8,8)); plt.imshow(x_art_adv_def[0][..., ::-1] / 255); plt.axis('off'); plt.show()\n",
        "\n",
        "\n",
        "ss = GaussianNoise()\n",
        "\n",
        "x_art_def = ss(x_test)\n",
        "x_art_adv_def = ss(x_test_adv_eps_5)\n",
        "\n",
        "# Compute the classifier predictions on the preprocessed inputs:\n",
        "pred_def = classifier.predict(x_art_def)\n",
        "label_def = np.argmax(pred_def, axis=1)[0]\n",
        "confidence_def = pred_def[:, label_def][0]\n",
        "\n",
        "pred_adv_def = classifier.predict(x_art_adv_def)\n",
        "label_adv_def = np.argmax(pred_adv_def, axis=1)[0]\n",
        "confidence_adv_def = pred_adv_def[:, label_adv_def][0]\n",
        "\n",
        "print(\"Defense: Gaussian Noise\")\n",
        "print('Prediction of original sample:', label_to_name(label_def), '- confidence {0:.2f}'.format(confidence_def))\n",
        "print('Prediction of adversarial sample:', label_to_name(label_adv_def), \n",
        "      '- confidence {0:.2f}'.format(confidence_adv_def))\n",
        "plt.figure(figsize=(8,8)); plt.imshow(x_art_adv_def[0][..., ::-1] / 255); plt.axis('off'); plt.show()\n",
        "\n",
        "ss = TotalVarMin(clip_values=(min_, max_))\n",
        "x_art_def, _ = ss(x_test)\n",
        "x_art_adv_def, _ = ss(x_test_adv_eps_5)\n",
        "\n",
        "# Compute the classifier predictions on the preprocessed inputs:\n",
        "pred_def = classifier.predict(x_art_def)\n",
        "label_def = np.argmax(pred_def, axis=1)[0]\n",
        "confidence_def = pred_def[:, label_def][0]\n",
        "\n",
        "pred_adv_def = classifier.predict(x_art_adv_def)\n",
        "label_adv_def = np.argmax(pred_adv_def, axis=1)[0]\n",
        "confidence_adv_def = pred_adv_def[:, label_adv_def][0]\n",
        "\n",
        "print(\"Defense: Total Variance Minimization\")\n",
        "print('Prediction of original sample:', label_to_name(label_def), '- confidence {0:.2f}'.format(confidence_def))\n",
        "print('Prediction of adversarial sample:', label_to_name(label_adv_def), \n",
        "      '- confidence {0:.2f}'.format(confidence_adv_def))\n",
        "plt.figure(figsize=(8,8)); plt.imshow(x_art_adv_def[0][..., ::-1] / 255); plt.axis('off'); plt.show()\n",
        "\n",
        "\n",
        "ss = ClassLabels()\n",
        "\n",
        "x_art_def = ss(x_test)\n",
        "x_art_adv_def = ss(x_test_adv_eps_5)\n",
        "\n",
        "# Compute the classifier predictions on the preprocessed inputs:\n",
        "pred_def = classifier.predict(x_art_def)\n",
        "label_def = np.argmax(pred_def, axis=1)[0]\n",
        "confidence_def = pred_def[:, label_def][0]\n",
        "\n",
        "pred_adv_def = classifier.predict(x_art_adv_def)\n",
        "label_adv_def = np.argmax(pred_adv_def, axis=1)[0]\n",
        "confidence_adv_def = pred_adv_def[:, label_adv_def][0]\n",
        "\n",
        "print(\"Defense: Class Labels Compression \")\n",
        "print('Prediction of original sample:', label_to_name(label_def), '- confidence {0:.2f}'.format(confidence_def))\n",
        "print('Prediction of adversarial sample:', label_to_name(label_adv_def), \n",
        "      '- confidence {0:.2f}'.format(confidence_adv_def))\n",
        "plt.figure(figsize=(8,8)); plt.imshow(x_art_adv_def[0][..., ::-1] / 255); plt.axis('off'); plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}