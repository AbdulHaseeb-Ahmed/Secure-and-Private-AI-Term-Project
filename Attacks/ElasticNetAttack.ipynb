{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5ElasticNetAttack.ipynb","provenance":[{"file_id":"1wNwZCni24FKNMkBY3mBnYkQ778kszHbA","timestamp":1586726942276},{"file_id":"https://github.com/podschwadt/teaching/blob/master/defend_cnn.ipynb","timestamp":1582598056181}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vsvgCPv17t3u","colab_type":"code","outputId":"29ba1c6d-4e6b-4e5b-e300-49a82103e0ea","executionInfo":{"status":"ok","timestamp":1587993767896,"user_tz":240,"elapsed":21466,"user":{"displayName":"Harnoor Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFpNkqY6f5Zk8692L_ARd3SVyrAmRKxF0IMyc8_w=s64","userId":"00087109453484129771"}},"colab":{"base_uri":"https://localhost:8080/","height":765}},"source":["%tensorflow_version 1.x\n","!pip install adversarial-robustness-toolbox\n","!git clone https://github.com/tensorflow/cleverhans.git\n","!pip install cleverhans/\n","!pip install cma"],"execution_count":3,"outputs":[{"output_type":"stream","text":["TensorFlow is already loaded. Please restart the runtime to change versions.\n","Requirement already satisfied: adversarial-robustness-toolbox in /usr/local/lib/python3.6/dist-packages (1.2.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox) (1.12.0)\n","Requirement already satisfied: Pillow==7.0.0 in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox) (7.0.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox) (3.2.1)\n","Requirement already satisfied: scikit-learn==0.22.1 in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox) (0.22.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox) (1.4.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox) (46.1.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox) (1.18.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox) (2.8.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.1->adversarial-robustness-toolbox) (0.14.1)\n","fatal: destination path 'cleverhans' already exists and is not an empty directory.\n","Processing ./cleverhans\n","Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (1.3.7)\n","Requirement already satisfied: pycodestyle in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (2.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (1.4.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (3.2.1)\n","Requirement already satisfied: mnist~=0.2 in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (0.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (1.18.3)\n","Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (0.10.0rc0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from cleverhans==3.0.1) (0.14.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans==3.0.1) (1.2.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans==3.0.1) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans==3.0.1) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans==3.0.1) (2.4.7)\n","Requirement already satisfied: cloudpickle>=1.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans==3.0.1) (1.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans==3.0.1) (1.12.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans==3.0.1) (4.4.2)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans==3.0.1) (0.3.3)\n","Building wheels for collected packages: cleverhans\n","  Building wheel for cleverhans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cleverhans: filename=cleverhans-3.0.1-cp36-none-any.whl size=253453 sha256=7a4ba96986a097fb0613796e4eac0abc01aad09654fccdd99026d63c6277fe4f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-hej6d5b6/wheels/d1/6b/1d/5cf7b3ca4c0cfc7f845628b8ed46366ab5f4f56b5483e9db7f\n","Successfully built cleverhans\n","Installing collected packages: cleverhans\n","  Found existing installation: cleverhans 3.0.1\n","    Uninstalling cleverhans-3.0.1:\n","      Successfully uninstalled cleverhans-3.0.1\n","Successfully installed cleverhans-3.0.1\n","Requirement already satisfied: cma in /usr/local/lib/python3.6/dist-packages (3.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cma) (1.18.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CT3K5k-fIIZ0","colab_type":"code","outputId":"8853eac4-9ab4-40ac-b7f8-ae17473380af","executionInfo":{"status":"ok","timestamp":1588002750879,"user_tz":240,"elapsed":1313271,"user":{"displayName":"Harnoor Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFpNkqY6f5Zk8692L_ARd3SVyrAmRKxF0IMyc8_w=s64","userId":"00087109453484129771"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1P34OVHlWadHxn6JPRLdek4GGULA6VB1e"}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","import keras\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from art.attacks import ElasticNet\n","from art.classifiers import KerasClassifier\n","from art.utils import load_dataset\n","import random\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","tf.compat.v1.disable_eager_execution()\n","\n","\n","\n","# Step 1: Load the CIFAR-10 Dataset\n","(x_train, y_train), (x_test, y_test), min_, max_ = load_dataset(str(\"cifar10\")) # Original Dataset\n","print(\"x_train shape: \" + str(x_train.shape) + \"\\n\" + \"x_train size: \" + str(x_train.size) + \"\\n\" + # this print statement is used for understanding what the CIFAR-10 dataset is\n","      \"y_train shape: \" + str(y_train.shape) + \"\\n\" + \"y_train size: \" + str(y_train.size) + \"\\n\" +\n","      \"x_test shape: \" + str(x_test.shape) + \"\\n\" + \"x_test size: \" + str(x_test.size) + \"\\n\" +\n","      \"y_test shape: \" + str(y_test.shape) + \"\\n\" + \"y_test size: \" + str(y_test.size) + \"\\n\")\n","print()\n","\n","\n","\n","# Step 2: Load the victim model\n","classifier_url =\"https://tfhub.dev/deepmind/ganeval-cifar10-convnet/1\" #@param {type:\"string\"} # model is downloaded from this site\n","IMAGE_SHAPE = (32, 32) # the image shape is needed so that the model knows the input-shape and since we are working with the CIFAR-10 all the images are 32 x 32 color images\n","classifier = KerasClassifier(model=tf.keras.Sequential([hub.KerasLayer(classifier_url, input_shape=IMAGE_SHAPE+(3,))]), clip_values=(min_, max_)) # this bascially creates a keras wrapper around the downloaded model so that we can use it with keras functions.\n","\n","\n","\n","# Step 3: Evaluate the victim model on the benign dataset\n","predictions = classifier.predict(x_test) # giving the classifier the x_test of the CIFAR-10 dataset.\n","accuracy_benign = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test) # calculates the accuracy of the predictions\n","print(\"Accuracy on benign test examples: {}%\\n\".format(accuracy_benign * 100))\n","\n","\n","\n","# Step 4: Collect 10 instances of each class from test set\n","def exract_ten_classes(data, labels, classes=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), no_instance=10):\n","    x_pre = []  # list to collect the x_test set\n","    y_pre = []  # list to collect the y_test set\n","    for class_label in range(0, 10):  # loop through each of the classes\n","        index = random.randint(0, 5000)  # choose an index from the x_test\n","        iteration = no_instance  # number of instance of each class to collect\n","        while (iteration != 0):\n","            if np.argmax(labels[index]) == classes[class_label]:  # check if the current index label matches the specified class label we are looking for\n","                x_pre.append(data[index])  # add the image to the x_test set\n","                y_pre.append(int(class_label))  # add the image label to the y_test set\n","                iteration = iteration - 1  # reduce # of instances by 1\n","            index = index + 1  # go to next index till next label is of the current class\n","    x = np.asarray(x_pre)  # append all 100, 10 of each class, images together\n","    y = keras.utils.to_categorical(np.asarray(y_pre), 10)  # append all 100, 10 of each class, labels together and do one hot encoding\n","    return x, y\n","\n","x_test_adv_pre, y_test_adv = exract_ten_classes( x_test, y_test )\n","print(\"x_test_adv_pre shape: \" + str(x_test_adv_pre.shape) + \"\\n\" + \"x_test_adv_pre size: \" + str(x_test_adv_pre.size) + \"\\n\" +\n","      \"y_test_adv_pre shape: \" + str(y_test_adv.shape) + \"\\n\" + \"y_test_adv_pre size: \" + str(y_test_adv.size) + \"\\n\")\n","\n","\n","# Step 6: Generate adversarial test examples and Evaluate the ART classifier on adversarial test examples\n","attack_eps_5 = ElasticNet(classifier=classifier, confidence=0.05, max_iter=100)\n","x_test_adv_eps_5 = attack_eps_5.generate(x=x_test_adv_pre)\n","predictions_eps_5 = classifier.predict(x_test_adv_eps_5)\n","accuracy_adv_eps_5 = np.sum(np.argmax(predictions_eps_5, axis=1) == np.argmax(y_test_adv, axis=1)) / len(y_test_adv)\n","#print(\"Accuracy on adversarial test examples with eps = 0.05: {}%\".format(accuracy_adv_eps_5 * 100))\n","\n","attack_eps_10 = ElasticNet(classifier=classifier, confidence=0.1, max_iter=100)\n","x_test_adv_eps_10 = attack_eps_10.generate(x=x_test_adv_pre)\n","predictions_eps_10 = classifier.predict(x_test_adv_eps_10)\n","accuracy_adv_eps_10 = np.sum(np.argmax(predictions_eps_10, axis=1) == np.argmax(y_test_adv, axis=1)) / len(y_test_adv)\n","#print(\"Accuracy on adversarial test examples with eps = 0.1: {}%\".format(accuracy_adv_eps_10 * 100))\n","\n","attack_eps_50 = ElasticNet(classifier=classifier, confidence=0.5, max_iter=100)\n","x_test_adv_eps_50 = attack_eps_50.generate(x=x_test_adv_pre)\n","predictions_eps_50 = classifier.predict(x_test_adv_eps_50)\n","accuracy_adv_eps_50 = np.sum(np.argmax(predictions_eps_50, axis=1) == np.argmax(y_test_adv, axis=1)) / len(y_test_adv)\n","#print(\"Accuracy on adversarial test examples with eps = 0.5: {}%\".format(accuracy_adv_eps_50 * 100))\n","\n","attack_eps_95 = ElasticNet(classifier=classifier, confidence=0.95, max_iter=100)\n","x_test_adv_eps_95 = attack_eps_95.generate(x=x_test_adv_pre)\n","predictions_eps_95 = classifier.predict(x_test_adv_eps_95)\n","accuracy_adv_eps_95 = np.sum(np.argmax(predictions_eps_95, axis=1) == np.argmax(y_test_adv, axis=1)) / len(y_test_adv)\n","#print(\"Accuracy on adversarial test examples with eps = 0.95: {}%\".format(accuracy_adv_eps_95 * 100))\n","\n","accuracies = [accuracy_adv_eps_5 * 100, accuracy_adv_eps_10 * 100, accuracy_adv_eps_50 * 100, accuracy_adv_eps_95 * 100]\n","\n","\n","# Step 7: Plot Results\n","for ind in range(0, 100, 5):\n","    fig = plt.figure(figsize=(16, 16))\n","    fig.suptitle('Adversarial Attack On Victim Model', fontsize=24, fontweight='bold')\n","    columns = 5\n","    rows = 7\n","    ax = []\n","\n","    ax.append(fig.add_subplot(rows, columns, 1))\n","    plt.text(0.38, 0.1, 'Original Image', fontsize=10, fontweight='bold')\n","    plt.axis('off')\n","\n","    eps = [0.05, 0.1, 0.5, 0.95]\n","    for i in range(2, 6):\n","        ax.append(fig.add_subplot(rows, columns, i))\n","        plt.text(0.0, 0.1, 'Adversarial Image Conf = ' + str(eps[i - 2]), fontsize=10, fontweight='bold')\n","        plt.axis('off')\n","\n","    imageindex = ind\n","    for i in range(5, columns*rows - 6, 5):\n","        sample_pre = x_test_adv_pre[ imageindex, :]\n","        ax.append( fig.add_subplot(rows, columns, i + 1) )\n","        label_pre = np.argmax(classifier.predict(sample_pre.reshape((1, sample_pre.shape[0], sample_pre.shape[1], sample_pre.shape[2]))))\n","        plt.text(33, 18, 'Data:\\nTrue Label = %d\\nPredicted Label = %d' % (np.argmax(y_test_adv[imageindex]), label_pre))\n","        plt.imshow(sample_pre)\n","\n","        sample_post_eps_5 = x_test_adv_eps_5[ imageindex, :]\n","        ax.append( fig.add_subplot(rows, columns, i + 2) )\n","        label_post_eps_5 = np.argmax(classifier.predict(sample_post_eps_5.reshape((1, sample_post_eps_5.shape[0], sample_post_eps_5.shape[1], sample_post_eps_5.shape[2]))))\n","        plt.text(33, 18, 'Data:\\nTrue Label = %d\\nPredicted Label = %d' % (np.argmax(y_test_adv[imageindex]), label_post_eps_5))\n","        plt.imshow(sample_post_eps_5)\n","\n","        sample_post_eps_10 = x_test_adv_eps_10[imageindex, :]\n","        ax.append(fig.add_subplot(rows, columns, i + 3))\n","        label_post_eps_10 = np.argmax(classifier.predict(sample_post_eps_10.reshape((1, sample_post_eps_10.shape[0], sample_post_eps_10.shape[1], sample_post_eps_10.shape[2]))))\n","        plt.text(33, 18, 'Data:\\nTrue Label = %d\\nPredicted Label = %d' % (np.argmax(y_test_adv[imageindex]), label_post_eps_10))\n","        plt.imshow(sample_post_eps_10)\n","\n","        sample_post_eps_50 = x_test_adv_eps_50[imageindex, :]\n","        ax.append(fig.add_subplot(rows, columns, i + 4))\n","        label_post_eps_50 = np.argmax(classifier.predict(sample_post_eps_50.reshape((1, sample_post_eps_50.shape[0], sample_post_eps_50.shape[1], sample_post_eps_50.shape[2]))))\n","        plt.text(33, 18, 'Data:\\nTrue Label = %d\\nPredicted Label = %d' % (np.argmax(y_test_adv[imageindex]), label_post_eps_50))\n","        plt.imshow(sample_post_eps_50)\n","\n","        sample_post_eps_95 = x_test_adv_eps_95[imageindex, :]\n","        ax.append(fig.add_subplot(rows, columns, i + 5))\n","        label_post_eps_95 = np.argmax(classifier.predict(sample_post_eps_95.reshape((1, sample_post_eps_95.shape[0], sample_post_eps_95.shape[1], sample_post_eps_95.shape[2]))))\n","        plt.text(33, 18, 'Data:\\nTrue Label = %d\\nPredicted Label = %d' % (np.argmax(y_test_adv[imageindex]), label_post_eps_95))\n","        plt.imshow(sample_post_eps_95)\n","\n","        imageindex = imageindex + 1\n","\n","\n","    ax.append(fig.add_subplot(rows, columns, 31))\n","    plt.text(0.0, 0.5, \"Accuracy on benign test examples: {}%\".format(round(accuracy_benign * 100),2), fontsize=8, fontweight='bold')\n","    plt.axis('off')\n","\n","    for i in range(32, 36):\n","        ax.append(fig.add_subplot(rows, columns, i))\n","        plt.text(0.0, 0.5, \"Accuracy on test examples Conf = \" + str(eps[i-32]) + \": {}%\".format(round(accuracies[i-32]), 2), fontsize=8, fontweight='bold')\n","        plt.axis('off')\n","\n","    fig.tight_layout(h_pad=4.0, w_pad=4.0)\n","    plt.show()\n","\n","\n","# Step 9: Data from Results\n","print()\n","print(\"Accuracy on benign test examples: {}%\".format(accuracy_benign * 100))\n","print(\"Accuracy on adversarial test examples with Conf = 0.05: {}%\".format(accuracies[0]))\n","print(\"Accuracy on adversarial test examples with Conf = 0.1: {}%\".format(accuracies[1]))\n","print(\"Accuracy on adversarial test examples with Conf = 0.5: {}%\".format(accuracies[2]))\n","print(\"Accuracy on adversarial test examples with Conf = 0.95: {}%\".format(accuracies[3]))\n","print()\n","\n","all_count = []\n","for j in range(0, 100, 10):\n","    count = [0, 0, 0, 0, 0]\n","    for i in range(j, j + 10):\n","        sample_pre = x_test_adv_pre[ i, : ]\n","        label_pre = np.argmax(classifier.predict(sample_pre.reshape((1, sample_pre.shape[0], sample_pre.shape[1], sample_pre.shape[2]))))\n","        sample_post_eps_5 = x_test_adv_eps_5[ i, : ]\n","        label_post_eps_5 = np.argmax(classifier.predict(sample_post_eps_5.reshape((1, sample_post_eps_5.shape[0], sample_post_eps_5.shape[1], sample_post_eps_5.shape[2]))))\n","        sample_post_eps_10 = x_test_adv_eps_10[ i, : ]\n","        label_post_eps_10 = np.argmax(classifier.predict(sample_post_eps_10.reshape((1, sample_post_eps_10.shape[0], sample_post_eps_10.shape[1], sample_post_eps_10.shape[2]))))\n","        sample_post_eps_50 = x_test_adv_eps_50[ i, : ]\n","        label_post_eps_50 = np.argmax(classifier.predict(sample_post_eps_50.reshape((1, sample_post_eps_50.shape[0], sample_post_eps_50.shape[1], sample_post_eps_50.shape[2]))))\n","        sample_post_eps_95 = x_test_adv_eps_95[ i, : ]\n","        label_post_eps_95 = np.argmax(classifier.predict(sample_post_eps_95.reshape((1, sample_post_eps_95.shape[0], sample_post_eps_95.shape[1], sample_post_eps_95.shape[2]))))\n","        if (label_pre == np.argmax(y_test_adv[i])):\n","            count[0] = count[0] + 1\n","        if (label_post_eps_5 == np.argmax(y_test_adv[i])):\n","            count[1] = count[1] + 1\n","        if (label_post_eps_10 == np.argmax(y_test_adv[i])):\n","            count[2] = count[2] + 1\n","        if (label_post_eps_50 == np.argmax(y_test_adv[i])):\n","            count[3] = count[3] + 1\n","        if (label_post_eps_95 == np.argmax(y_test_adv[i])):\n","            count[4] = count[4] + 1\n","    all_count.append(count)\n","print()\n","\n","Labels = [\"Airplane\", \"Automobile\", \"Bird\", \"Cat\", \"Deer\", \"Dog\", \"Frog\", \"Horse\", \"Ship\", \"Truck\"]\n","for i in range(0, 10):\n","    print(\"Classifier with benign example has \" + str(Labels[i]) + \" recognition accuracy of = \" + str((all_count[i][0] / 10) * 100) + \"%\")\n","    print(\"Elastic Net Method with Conf = 0.05 has \" + str(Labels[i]) + \" recognition accuracy of = \" + str((all_count[i][1] / 10) * 100) + \"%\")\n","    print(\"Elastic Net Method with Conf = 0.10 has \" + str(Labels[i]) + \" recognition accuracy of = \" + str((all_count[i][2] / 10) * 100) + \"%\")\n","    print(\"Elastic Net Method with Conf = 0.50 has \" + str(Labels[i]) + \" recognition accuracy of = \" + str((all_count[i][3] / 10) * 100) + \"%\")\n","    print(\"Elastic Net Method with Conf = 0.95 has \" + str(Labels[i]) + \" recognition accuracy of = \" + str((all_count[i][4] / 10) * 100) + \"%\")\n","    print()"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"0hDSF2uO518Q","colab_type":"code","outputId":"e6433041-fbc4-48a4-963a-31ceb8af2b5c","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install git+https://github.com/nottombrown/imagenet_stubs\n","\n","import imagenet_stubs\n","from imagenet_stubs.imagenet_2012_labels import name_to_label, label_to_name\n","from art.defences import JpegCompression\n","from art.defences import TotalVarMin\n","from art.defences import GaussianNoise\n","from art.defences import PixelDefend\n","from art.defences import ClassLabels\n","\n","ss = JpegCompression(clip_values=(min_, max_))\n","\n","x_art_def, _ = ss(x_test)\n","x_art_adv_def, _ = ss(x_test_adv_eps_5)\n","\n","# Compute the classifier predictions on the preprocessed inputs:\n","pred_def = classifier.predict(x_art_def)\n","label_def = np.argmax(pred_def, axis=1)[0]\n","confidence_def = pred_def[:, label_def][0]\n","\n","pred_adv_def = classifier.predict(x_art_adv_def)\n","label_adv_def = np.argmax(pred_adv_def, axis=1)[0]\n","confidence_adv_def = pred_adv_def[:, label_adv_def][0]\n","\n","print(\"Defense: JPEG Compression \")\n","print('Prediction of original sample:', label_to_name(label_def), '- confidence {0:.2f}'.format(confidence_def))\n","print('Prediction of adversarial sample:', label_to_name(label_adv_def), \n","      '- confidence {0:.2f}'.format(confidence_adv_def))\n","plt.figure(figsize=(8,8)); plt.imshow(x_art_adv_def[0][..., ::-1] / 255); plt.axis('off'); plt.show()\n","\n","\n","ss = GaussianNoise()\n","\n","x_art_def = ss(x_test)\n","x_art_adv_def = ss(x_test_adv_eps_5)\n","\n","# Compute the classifier predictions on the preprocessed inputs:\n","pred_def = classifier.predict(x_art_def)\n","label_def = np.argmax(pred_def, axis=1)[0]\n","confidence_def = pred_def[:, label_def][0]\n","\n","pred_adv_def = classifier.predict(x_art_adv_def)\n","label_adv_def = np.argmax(pred_adv_def, axis=1)[0]\n","confidence_adv_def = pred_adv_def[:, label_adv_def][0]\n","\n","print(\"Defense: Gaussian Noise\")\n","print('Prediction of original sample:', label_to_name(label_def), '- confidence {0:.2f}'.format(confidence_def))\n","print('Prediction of adversarial sample:', label_to_name(label_adv_def), \n","      '- confidence {0:.2f}'.format(confidence_adv_def))\n","plt.figure(figsize=(8,8)); plt.imshow(x_art_adv_def[0][..., ::-1] / 255); plt.axis('off'); plt.show()\n","\n","ss = TotalVarMin(clip_values=(min_, max_))\n","x_art_def, _ = ss(x_test)\n","x_art_adv_def, _ = ss(x_test_adv_eps_5)\n","\n","# Compute the classifier predictions on the preprocessed inputs:\n","pred_def = classifier.predict(x_art_def)\n","label_def = np.argmax(pred_def, axis=1)[0]\n","confidence_def = pred_def[:, label_def][0]\n","\n","pred_adv_def = classifier.predict(x_art_adv_def)\n","label_adv_def = np.argmax(pred_adv_def, axis=1)[0]\n","confidence_adv_def = pred_adv_def[:, label_adv_def][0]\n","\n","print(\"Defense: Total Variance Minimization\")\n","print('Prediction of original sample:', label_to_name(label_def), '- confidence {0:.2f}'.format(confidence_def))\n","print('Prediction of adversarial sample:', label_to_name(label_adv_def), \n","      '- confidence {0:.2f}'.format(confidence_adv_def))\n","plt.figure(figsize=(8,8)); plt.imshow(x_art_adv_def[0][..., ::-1] / 255); plt.axis('off'); plt.show()\n","\n","\n","ss = ClassLabels()\n","\n","x_art_def = ss(x_test)\n","x_art_adv_def = ss(x_test_adv_eps_5)\n","\n","# Compute the classifier predictions on the preprocessed inputs:\n","pred_def = classifier.predict(x_art_def)\n","label_def = np.argmax(pred_def, axis=1)[0]\n","confidence_def = pred_def[:, label_def][0]\n","\n","pred_adv_def = classifier.predict(x_art_adv_def)\n","label_adv_def = np.argmax(pred_adv_def, axis=1)[0]\n","confidence_adv_def = pred_adv_def[:, label_adv_def][0]\n","\n","print(\"Defense: Class Labels Compression \")\n","print('Prediction of original sample:', label_to_name(label_def), '- confidence {0:.2f}'.format(confidence_def))\n","print('Prediction of adversarial sample:', label_to_name(label_adv_def), \n","      '- confidence {0:.2f}'.format(confidence_adv_def))\n","plt.figure(figsize=(8,8)); plt.imshow(x_art_adv_def[0][..., ::-1] / 255); plt.axis('off'); plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/nottombrown/imagenet_stubs\n","  Cloning https://github.com/nottombrown/imagenet_stubs to /tmp/pip-req-build-_2sx9701\n","  Running command git clone -q https://github.com/nottombrown/imagenet_stubs /tmp/pip-req-build-_2sx9701\n","Building wheels for collected packages: imagenet-stubs\n","  Building wheel for imagenet-stubs (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imagenet-stubs: filename=imagenet_stubs-0.0.7-cp36-none-any.whl size=794841 sha256=4fc4c1202aa15ad00cd2dfb7643f18b6bf0be84673b09e2ac9ed0a6a6f5ee099\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-w7jh241i/wheels/ae/51/e7/c1cfa8692ea864410aa778b169fc7766af073ac52a1bc23301\n","Successfully built imagenet-stubs\n","Installing collected packages: imagenet-stubs\n","Successfully installed imagenet-stubs-0.0.7\n","Defense: JPEG Compression \n","Prediction of original sample: tiger shark, Galeocerdo cuvieri - confidence 6.12\n","Prediction of adversarial sample: hen - confidence 10.20\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x576 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["Defense: Gaussian Noise\n","Prediction of original sample: stingray - confidence 8.51\n","Prediction of adversarial sample: stingray - confidence 10.40\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAcEAAAHBCAYAAAARuwDoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAKTUlEQVR4nO3dwY6jRhRAUYjm/z85ZO0og9Jv7DJwz9mOWuCi7CsW82o/jmMDgKK/vn0DAPAtIghAlggCkCWCAGSJIABZIghA1q+zf9z3ffb/J/bJnwz+aNu2Yxvc4jG71nQ5Zv8LZXiPg/WY/ieZ2R3OrrcvvNgd/tPQ0vUYXmu6gyfm/9Xr59cb/w6M/mjp4k9/rEYme/iY7o+///7tH3oTBCBLBAHIEkEAskQQgCwRBCBLBAHIEkEAskQQgCwRBCBLBAHIEkEAskQQgCwRBCBrP5u+Pj5F4uJWnn7A/dgfX2TxX1iO9ziO3x/H4U0QgCwRBCBLBAHIEkEAskQQgCwRBCBLBAHIEkEAskQQgCwRBCBLBAHIEkEAsn59+wb+1GTArOGy73GH4b4r94e9+AYWhMW8CQKQJYIAZIkgAFkiCECWCAKQJYIAZIkgAFkiCECWCAKQJYIAZIkgAFkiCECWCAKQdftTJK5+IsG2PXcw/h1OW3jq2tNg/36eN0EAskQQgCwRBCBLBAHIEkEAskQQgCwRBCBLBAHIEkEAskQQgCwRBCBLBAHIuv0A7ZUMs32Pp67jUz/XUqbUs5g3QQCyRBCALBEEIEsEAcgSQQCyRBCALBEEIEsEAcgSQQCyRBCALBEEIEsEAcgSQQCyLnOKhOHx7zFZx/EaemhfM136iaWPy97IWPpbdcKbIABZIghAlggCkCWCAGSJIABZIghAlggCkCWCAGSJIABZIghAlggCkCWCAGSJIABZlzlFwvD49zDxv8HSc3dX2cPeBAHIEkEAskQQgCwRBCBLBAHIEkEAskQQgCwRBCBLBAHIEkEAskQQgCwRBCDrMgO0n2wf/M1VhsvC1Mp9P7nWlO/mq+naX2UdvQkCkCWCAGSJIABZIghAlggCkCWCAGSJIABZIghAlggCkCWCAGSJIABZIghAlggCkOUUiQWuMi0dVlq5733HvmflyR+feM7eBAHIEkEAskQQgCwRBCBLBAHIEkEAskQQgCwRBCBLBAHIEkEAskQQgCwRBCDLAG0e7SpDerkm++N7rrKO3gQByBJBALJEEIAsEQQgSwQByBJBALJEEIAsEQQgSwQByBJBALJEEIAsEQQgSwQByHKKBKNJ+tt2nSnwZ+5wj3yP/fE9++CH5/jAA/MmCECWCAKQJYIAZIkgAFkiCECWCAKQJYIAZIkgAFkiCECWCAKQJYIAZIkgAFkiCECWUyT4wiT9ybkV5v3fjsfMiU+cCDHhTRCALBEEIEsEAcgSQQCyRBCALBEEIEsEAcgSQQCyRBCALBEEIEsEAcgSQQCyTgdoT+bfbtv1Z+A+9XPdx89Xch8+tKsM6U166Nr7/XgWb4IAZIkgAFkiCECWCAKQJYIAZIkgAFkiCECWCAKQJYIAZIkgAFkiCECWCAKQJYIAZJ2eInHcYFz65BZNc/+ywUO7w2kQK/fiDb6aw/WYfbLxKSODFTmmP4zThzYx/cJMFvIOX84T3gQByBJBALJEEIAsEQQgSwQByBJBALJEEIAsEQQgSwQByBJBALJEEIAsEQQg63SA9h0mTd/gFvm3hz600Vzl4VrcYQkn67GvHsZ88Ye2T6duDyeKX30W9idmkHsTBCBLBAHIEkEAskQQgCwRBCBLBAHIEkEAskQQgCwRBCBLBAHIEkEAskQQgCwRBCDr/BSJp/rEKPIzF5/Mznvsk+c83YsL99TK7Ts61GH7g9MWLv7lPEabatv24ULug+uNV3Dwh594Wt4EAcgSQQCyRBCALBEEIEsEAcgSQQCyRBCALBEEIEsEAcgSQQCyRBCALBEEIOsyA7SXjr+99szc51s4wHw4f3hkthdNc38xvb2VD3ql4ceaD94e/M3oSjMGaAPAG4kgAFkiCECWCAKQJYIAZIkgAFkiCECWCAKQJYIAZIkgAFkiCECWCAKQJYIAZF3mFImHzoDnP0wm1Y/3xz6YcX9cfzfuC78xKw92mF7rBo9srenpE++9i1vwJghAlggCkCWCAGSJIABZIghAlggCkCWCAGSJIABZIghAlggCkCWCAGSJIABZIghA1ukpEoP5+9u2NSeRnxus5GQE/7bdYvGPycYaL8fP/3C670ePefjBpqcmTD6b3wE+YuHvwBlvggBkiSAAWSIIQJYIApAlggBkiSAAWSIIQJYIApAlggBkiSAAWSIIQJYIApB1OkDbANz32AfDsKcDkm9h8NnGQ5wH1xov/cJrGWrNR6wcan2RzehNEIAsEQQgSwQByBJBALJEEIAsEQQgSwQByBJBALJEEIAsEQQgSwQByBJBALJEEICs01Mknmr1BP5HnwixiCV8ZT3uZ+UBDWPBjeVNEIAsEQQgSwQByBJBALJEEIAsEQQgSwQByBJBALJEEIAsEQQgSwQByBJBALKuM0B74VTr4IxYeLbh78e+8PfD7841eRMEIEsEAcgSQQCyRBCALBEEIEsEAcgSQQCyRBCALBEEIEsEAcgSQQCyRBCALBEEIOs6p0gYsf5i4aEavMnkmT35ee2DBTkWH9Hw5PXn//EmCECWCAKQJYIAZIkgAFkiCECWCAKQJYIAZIkgAFkiCECWCAKQJYIAZIkgAFkiCEDWdU6R4IXp9vczeWZPPi1kfCIELORNEIAsEQQgSwQByBJBALJEEIAsEQQgSwQByBJBALJEEIAsEQQgSwQByBJBALIM0F5gMiTZ7GE+wV58ZT3wJghAlggCkCWCAGSJIABZIghAlggCkCWCAGSJIABZIghAlggCkCWCAGSJIABZIghAllMkFjB1nt9ZvTeeuhcnp0Fs29r1cGLFNXkTBCBLBAHIEkEAskQQgCwRBCBLBAHIEkEAskQQgCwRBCBLBAHIEkEAskQQgCwDtB/GkF7OPHV/uEemvAkCkCWCAGSJIABZIghAlggCkCWCAGSJIABZIghAlggCkCWCAGSJIABZIghAlggCkPWRUySeOqn+Du6wjvbH91hHeOVNEIAsEQQgSwQByBJBALJEEIAsEQQgSwQByBJBALJEEIAsEQQgSwQByBJBALJEEICsj5wiYVI9AHfgTRCALBEEIEsEAcgSQQCyRBCALBEEIEsEAcgSQQCyRBCALBEEIEsEAcgSQQCyPjJAG84YsM677cO/m+zFldfi87wJApAlggBkiSAAWSIIQJYIApAlggBkiSAAWSIIQJYIApAlggBkiSAAWSIIQJYIApDlFIkFJlPnTZzn7lbu+5XfF9/NZ/EmCECWCAKQJYIAZIkgAFkiCECWCAKQJYIAZIkgAFkiCECWCAKQJYIAZIkgAFmnA7T30QjcbTuWjpj9+T3u++z+juHHOgbLOLzF0bVWTwTeB/d4jD7Ytu2DD3cM9/3oWrNLjffH8GqjvzoW3uRwGcff6YnpPS41+m5OL7Xwh/GEN0EAskQQgCwRBCBLBAHIEkEAskQQgCwRBCBLBAHIEkEAskQQgCwRBCBLBAHIEkEAsvZj5Rh1ALgQb4IAZIkgAFkiCECWCAKQJYIAZIkgAFn/ALTyCnF+d85hAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 576x576 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Defense: Total Variance Minimization\n","Prediction of original sample: hen - confidence 6.57\n","Prediction of adversarial sample: hen - confidence 8.68\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x576 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}